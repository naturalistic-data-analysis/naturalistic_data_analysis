---
redirect_from:
  - "/features/notebooks/pliers-tutorial"
interact_link: content/features/notebooks/Pliers_Tutorial.ipynb
kernel_name: python3
kernel_path: content/features/notebooks
has_widgets: false
title: |-
  Automated Annotations
pagenum: 13
prev_page:
  url: /features/notebooks/HiddenSemiMarkovModel.html
next_page:
  url: /features/notebooks/hypertools.html
suffix: .ipynb
search: pliers audio example stimuli using features github stimulus extraction text conversion sherlock video feature extractors speech extractor tyarkoni transformers stim frames run extract transform api paranoiastory take io html videostim graph however lets rmsextractor convert want input faces complex analysis simply code track data s transcript naturalistic manually based not automatically extracted easy import object objects frame framesamplingfilter rev ai predefineddictionaryextractor sentiment dozens installation extracting converters apis manual range just into rms need pandas documentation our audiostim results face note also words com tutorial provides python interface services models algorithms well datasets visual where apply returns easily dataframe same result

comment: "***PROGRAMMATICALLY GENERATED, DO NOT EDIT. SEE ORIGINAL FILES IN /content***"
---

    <main class="jupyter-page">
    <div id="page-info"><div id="page-title">Automated Annotations</div>
</div>
    
<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Automated-Feature-Extraction-using-Pliers">Automated Feature Extraction using Pliers<a class="anchor-link" href="#Automated-Feature-Extraction-using-Pliers"> </a></h1><p><em>written by Alejandro de la Vega</em></p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This tutorial provides an introduction to the automated extraction of features from multi-modal naturalistic stimuli for use as regressors in event-related analyes. We will be using <em>pliers</em>, a Python library which provides a unified, standardized interface to dozens of different feature extraction tools and services, including state-of-the-art deep learning models.</p>
<h3 id="In-this-lab-we-will-cover:">In this lab we will cover:<a class="anchor-link" href="#In-this-lab-we-will-cover:"> </a></h3><ul>
<li>Installation and getting started</li>
<li>Pliers fundamentals</li>
<li>Extracting features</li>
<li>Converting stimuli across modalities</li>
<li>Chaining converters and feature extractors manually</li>
<li>Speech to Text Conversion using APIs</li>
<li>Speech / Text based extractors</li>
<li>Complex workflow managment using Pliers Graph API</li>
</ul>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="k">import</span> <span class="n">YouTubeVideo</span>

<span class="n">YouTubeVideo</span><span class="p">(</span><span class="s1">&#39;4mQjtyQPu_c&#39;</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">

        <iframe
            width="400"
            height="300"
            src="https://www.youtube.com/embed/4mQjtyQPu_c"
            frameborder="0"
            allowfullscreen
        ></iframe>
        
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Why-use-Pliers-for-automated-feature-extraction?">Why use Pliers for automated feature extraction?<a class="anchor-link" href="#Why-use-Pliers-for-automated-feature-extraction?"> </a></h1>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Naturalistic fMRI paradigms have gained popularity due to their potential to more closely resemble the complex, dynamic nature of real-world perception. Due to their very <em>nature</em>, however, these complex stimuli pose serious practical challenges to analyze. Manual annotation is effortful and time consuming, and does not scale to capture the wide range of perceptual dynamics present in these stimuli.</p>
<p>Fortunately, recent advancements in machine learning have made it possible to rapidly and automatically annotate multi-modal stimuli with a wide range of algorithms. Extracted features range from low-level perceptual features (such as brightness and loudness), to complex, psychological relevant features such as predictions from state-of-the-art language comprehension models. The timecourses of these extracted features can then be used as regressors in task-based fMRI analysis.</p>
<p>A consequence of the variety of potential features available, however, is that the interfaces to these various algorithms, and content analysis APIs are highly heterogenous. Simply installing and writing the code necessary to interface and harmonize these algorithms can be a substantial endeavour. <em>Pliers</em> aims to facilitate the process by providing a uniform interface to a wide range of tools under an easy to use and unified Python framework.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Installation">Installation<a class="anchor-link" href="#Installation"> </a></h1>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Installating the base <em>pliers</em> package is simple, just use pip:</p>

<pre><code>pip install -U pliers</code></pre>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>As <em>pliers</em> interfaces with dozens of external libraries, there are many <strong>optional dependencies</strong> that are not installed by default to keep the installation light.
<em>Pliers</em> will prompt you with installation instructions if you try to use an extractor that is not yet installed.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Naturalistic-Stimuli-from-ParanoiaStory-and-Sherlock">Naturalistic Stimuli from <code>ParanoiaStory</code> and <code>Sherlock</code><a class="anchor-link" href="#Naturalistic-Stimuli-from-ParanoiaStory-and-Sherlock"> </a></h3>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We'll be working with the first run stimuli from the <code>ParanoiaStory</code> and the <code>Sherlock</code> datasets.</p>
<p><code>ParanoiaStory</code> is an audio narrative, while <code>Sherlock</code> is a an audio-visual episode from a TV show.</p>
<p>Let's set up the paths of these stimuli and take a quick look at what we're dealing with. Make sure you change the two <code>data_dir</code> paths to where you have install the Paranoia and Sherlock datasets.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">datalad.api</span> <span class="k">as</span> <span class="nn">dl</span>

<span class="n">data_dir_paranoia</span> <span class="o">=</span> <span class="s1">&#39;/Volumes/Engram/Data/Paranoia/&#39;</span>
<span class="n">data_dir_sherlock</span> <span class="o">=</span> <span class="s1">&#39;/Volumes/Engram/Data/Sherlock/&#39;</span>

<span class="n">paranoia_audio</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">data_dir_paranoia</span><span class="p">,</span> <span class="s1">&#39;stimuli&#39;</span><span class="p">,</span> <span class="s1">&#39;stimuli_story1_audio.wav&#39;</span><span class="p">)</span>
<span class="n">sherlock_video</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">data_dir_sherlock</span><span class="p">,</span> <span class="s1">&#39;stimuli&#39;</span><span class="p">,</span><span class="s1">&#39;stimuli_Sherlock.m4v&#39;</span><span class="p">)</span>

<span class="c1"># If datasets haven&#39;t been installed, clone from GIN repository</span>
<span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">data_dir_paranoia</span><span class="p">):</span>
    <span class="n">dl</span><span class="o">.</span><span class="n">clone</span><span class="p">(</span><span class="n">source</span><span class="o">=</span><span class="s1">&#39;https://gin.g-node.org/ljchang/Paranoia&#39;</span><span class="p">,</span> <span class="n">path</span><span class="o">=</span><span class="n">data_dir_paranoia</span><span class="p">)</span>

<span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">data_dir_sherlock</span><span class="p">):</span>
    <span class="n">dl</span><span class="o">.</span><span class="n">clone</span><span class="p">(</span><span class="n">source</span><span class="o">=</span><span class="s1">&#39;https://gin.g-node.org/ljchang/Sherlock&#39;</span><span class="p">,</span> <span class="n">path</span><span class="o">=</span><span class="n">data_dir_sherlock</span><span class="p">)</span>

<span class="c1"># Initialize dataset</span>
<span class="n">ds_paranoia</span> <span class="o">=</span> <span class="n">dl</span><span class="o">.</span><span class="n">Dataset</span><span class="p">(</span><span class="n">data_dir_paranoia</span><span class="p">)</span>
<span class="n">ds_sherlock</span> <span class="o">=</span> <span class="n">dl</span><span class="o">.</span><span class="n">Dataset</span><span class="p">(</span><span class="n">data_dir_sherlock</span><span class="p">)</span>

<span class="c1"># Get Paranoia story</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">ds_paranoia</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">paranoia_audio</span><span class="p">)</span>

<span class="c1"># Get Sherlock video</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">ds_sherlock</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">sherlock_video</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Getting-Started">Getting Started<a class="anchor-link" href="#Getting-Started"> </a></h1><p>The best way to see what <em>pliers</em> can offer is to jump right into an example.</p>
<h2 id="Example-1:-Audio-RMS">Example 1: Audio RMS<a class="anchor-link" href="#Example-1:-Audio-RMS"> </a></h2><p>A measure that is likely to capture a large amount of variance in auditory cortex activity during naturalistic stimulation is the power of the signal in the auditory track. A simple way to measure this is by extracting the <code>Root-Mean-Square (RMS)</code> of the audio signal across time.</p>
<p><em>Pliers</em> makes this very easy. All we need to do is to import the <code>RMSExtractor</code> object, and apply it to the <code>ParanoiaStory</code> audio stimulus.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">pliers.extractors</span> <span class="k">import</span> <span class="n">RMSExtractor</span>

<span class="c1"># Create an instance of this extractor</span>
<span class="n">ext</span> <span class="o">=</span> <span class="n">RMSExtractor</span><span class="p">()</span>

<span class="c1"># Extract features from the audio stimulus</span>
<span class="n">rms_result</span> <span class="o">=</span> <span class="n">ext</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">paranoia_audio</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The Extractor returns an <code>ExtractorResult</code> object which contains the extracted values.</p>
<p>We can easily convert this to a <em>Pandas DataFrame</em>, a familar format that could easily be fed into a data-analysis pipeline, and is easy to inspect.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">rms_df</span> <span class="o">=</span> <span class="n">rms_result</span><span class="o">.</span><span class="n">to_df</span><span class="p">()</span>
<span class="n">rms_df</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>order</th>
      <th>duration</th>
      <th>onset</th>
      <th>object_id</th>
      <th>rms</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>0.01161</td>
      <td>0.000000</td>
      <td>0</td>
      <td>0.000743</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>0.01161</td>
      <td>0.011610</td>
      <td>0</td>
      <td>0.000776</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2</td>
      <td>0.01161</td>
      <td>0.023220</td>
      <td>0</td>
      <td>0.000823</td>
    </tr>
    <tr>
      <th>3</th>
      <td>3</td>
      <td>0.01161</td>
      <td>0.034830</td>
      <td>0</td>
      <td>0.001723</td>
    </tr>
    <tr>
      <th>4</th>
      <td>4</td>
      <td>0.01161</td>
      <td>0.046440</td>
      <td>0</td>
      <td>0.001852</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>45124</th>
      <td>45124</td>
      <td>0.01161</td>
      <td>523.888617</td>
      <td>0</td>
      <td>0.001723</td>
    </tr>
    <tr>
      <th>45125</th>
      <td>45125</td>
      <td>0.01161</td>
      <td>523.900227</td>
      <td>0</td>
      <td>0.001555</td>
    </tr>
    <tr>
      <th>45126</th>
      <td>45126</td>
      <td>0.01161</td>
      <td>523.911837</td>
      <td>0</td>
      <td>0.001433</td>
    </tr>
    <tr>
      <th>45127</th>
      <td>45127</td>
      <td>0.01161</td>
      <td>523.923447</td>
      <td>0</td>
      <td>0.001312</td>
    </tr>
    <tr>
      <th>45128</th>
      <td>45128</td>
      <td>0.01161</td>
      <td>523.935057</td>
      <td>0</td>
      <td>0.001174</td>
    </tr>
  </tbody>
</table>
<p>45129 rows × 5 columns</p>
</div>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can then easily plot the timeline of <code>rms</code> across time for the <em>ParanoiaStory</em> study.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">%</span><span class="k">matplotlib</span> inline
<span class="n">rms_df</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="s1">&#39;onset&#39;</span><span class="p">,</span> <span class="s1">&#39;rms&#39;</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x14520c5c0&gt;</pre>
</div>

</div>
</div>
<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="../../images/features/notebooks/Pliers_Tutorial_16_1.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can now do the same thing for the <em>Sherlock</em> video stimulus, using the same <code>RMSExtractor</code>.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">ext</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">sherlock_video</span><span class="p">)</span><span class="o">.</span><span class="n">to_df</span><span class="p">()</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="s1">&#39;onset&#39;</span><span class="p">,</span> <span class="s1">&#39;rms&#39;</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x148066898&gt;</pre>
</div>

</div>
</div>
<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="../../images/features/notebooks/Pliers_Tutorial_18_1.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>That was easy! We can now see that the RMS profile of <code>Sherlock</code> is very different from the <code>ParanoiaStory</code> narrative.</p>
<h5 id="But,-wait.-How-were-we-able-to-apply-RMSExtractor-to-a-video-when-this-is-an-audio--Extractor?">But, wait. How were we able to apply <code>RMSExtractor</code> to a <em>video</em> when this is an <em>audio</em>  Extractor?<a class="anchor-link" href="#But,-wait.-How-were-we-able-to-apply-RMSExtractor-to-a-video-when-this-is-an-audio--Extractor?"> </a></h5><p>Under the hood, <em>pliers</em> noticed this mismatch, and automatically converted the Sherlock video clip to an audio stimuli by extracting the audio track. As a result, the <code>RMSExtractor</code> <em>just worked</em> and returned to you the result you were expecting.</p>
<p>Before we can understand how <em>pliers</em> does this, and how you can do this with greater manual control, it's important to first understand some basic concepts.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Fundamentals-of-Pliers">Fundamentals of Pliers<a class="anchor-link" href="#Fundamentals-of-Pliers"> </a></h1><p>This section is adapted from the <a href="http://tyarkoni.github.io/pliers/basic-concepts.html"><em>pliers</em> documentation</a>. For a more in-depth dive, see the official documentation.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Stims-and-Transformers">Stims and Transformers<a class="anchor-link" href="#Stims-and-Transformers"> </a></h2><p><em>Pliers</em> is deliberately designed with simplicity in mind, and loosely modeled after usage patterns in <em>scikit-learn</em>.</p>
<p>At its core, <em>pliers</em> is based around two kinds of objects: the <code>Stim</code>, and the <code>Transformer</code>.</p>
<p>A <code>Stim</code> is a container for a data object we want to extract features from-- for example, the <code>Sherlock</code> video clip or the <code>ParanoiaStory</code> audio clip.</p>
<p>If you pass a string path to the <code>transform</code> method of an <code>Extractor</code>, <em>pliers</em> will attempt to automatically load the stimulus. This automatical detection is not fool proof, however, so explicilty loading the stimuli can be useful.</p>
<p>A <code>Transformer</code> provides functionality to <em>do something with</em> <code>Stims</code>–either to change the input Stim in some way (e.g., converting a <code>Stim</code> from video to audio), or to extract feature data from it (such as extracting <code>rms</code> from an audio stimulus).</p>
<p>Let's modify the previous example to explicitly load our stimuli:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">pliers.stimuli</span> <span class="k">import</span> <span class="n">AudioStim</span>
<span class="kn">from</span> <span class="nn">pliers.extractors</span> <span class="k">import</span> <span class="n">RMSExtractor</span>

<span class="n">stim</span> <span class="o">=</span> <span class="n">AudioStim</span><span class="p">(</span><span class="n">paranoia_audio</span><span class="p">)</span>
<span class="n">ext</span> <span class="o">=</span> <span class="n">RMSExtractor</span><span class="p">()</span>
<span class="n">rms_features</span> <span class="o">=</span> <span class="n">ext</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">stim</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can see the separation between the target of the feature extraction (the <em>ParanoiaStory</em> <code>AudioStim</code>), and the <code>RMSExtractor</code> object that <em>transforms</em> this stimulus into an <code>ExtractorResult</code>.</p>
<p>This basic pattern persists throughout <em>pliers</em>, no matter how many Transformers we string together, or how many Stim inputs we feed in.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Types-of-Transformers">Types of Transformers<a class="anchor-link" href="#Types-of-Transformers"> </a></h3>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>There are three types of <code>Transformers</code> in <em>pliers</em>, and all take <code>Stim</code> objects as inputs.</p>
<ul>
<li><p><code>Extractors</code> return extracted feature data as <code>ExtractorResult</code> objects (which can be converted to a pandas dataframes).</p>
<p>We have already seen an example of this with <code>RMSExtractor</code>, but there are dozens more in <em>pliers</em>.</p>
</li>
</ul>
<ul>
<li><p><code>Converters</code> take a <code>Stim</code>, and convert it to a different type of <code>Stim</code>.</p>
<p>For example, the <code>VideoToAudioConverter</code> takes a <code>VideoStim</code> as input, and returns an <code>AudioStim</code> with ony the audio track as output.</p>
</li>
</ul>
<ul>
<li><p><code>Filters</code> take a <code>Stim</code>, and return a <em>modified</em> <code>Stim</code> of the <em>same type</em>.</p>
<p>For example, the <code>ImageCroppingFilter</code> takes an <code>ImageStim</code> as input, and returns another <code>ImageStim</code> as output, where the image data stored in the <code>Stim</code> has been cropped by a bounding box specified by the user.</p>
</li>
</ul>
<p>A <a href="http://tyarkoni.github.io/pliers/transformers.html#">complete listing</a> of all <code>Transformers</code> is available in the offical <em>pliers</em> documentation.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In practice, users will <strong>primarily interact</strong> with <code>Extractors</code>, as <em>pliers</em> will automatically convert stimuli prior to extraction, greatly reducing the complexity of the code.</p>
<p>However, it's often useful to manually combine these operations to fully control the process.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Example-2:-Manual-stimulus-conversion-+-RMSExtractor">Example 2: Manual stimulus conversion + RMSExtractor<a class="anchor-link" href="#Example-2:-Manual-stimulus-conversion-+-RMSExtractor"> </a></h2>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In <strong>Example 1</strong>, we applied the <code>RMSExtractor</code> to a the <code>Sherlock</code> video stimulus, and <em>pliers</em> automatically converted the stimulus from <code>VideoStim</code> to an <code>AudioStim</code> containing only the audio track.</p>
<p>Let's modify this example, to use manual conversion from Video to Audio:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">pliers.converters</span> <span class="k">import</span> <span class="n">VideoToAudioConverter</span>
<span class="kn">from</span> <span class="nn">pliers.extractors</span> <span class="k">import</span> <span class="n">RMSExtractor</span>

<span class="c1"># Set up Extractor and Converter</span>
<span class="n">conv</span> <span class="o">=</span> <span class="n">VideoToAudioConverter</span><span class="p">()</span>
<span class="n">ext</span> <span class="o">=</span> <span class="n">RMSExtractor</span><span class="p">()</span>

<span class="c1"># Transform Video to Audio</span>
<span class="n">audio_only</span> <span class="o">=</span> <span class="n">conv</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">sherlock_video</span><span class="p">)</span>
<span class="c1"># Transform Video to RMS results</span>
<span class="n">audio_result</span> <span class="o">=</span> <span class="n">ext</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">audio_only</span><span class="p">)</span>

<span class="n">audio_result</span><span class="o">.</span><span class="n">to_df</span><span class="p">()</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="s1">&#39;onset&#39;</span><span class="p">,</span> <span class="s1">&#39;rms&#39;</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x14920f7b8&gt;</pre>
</div>

</div>
</div>
<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="../../images/features/notebooks/Pliers_Tutorial_29_1.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>As you can see, the results are identical to letting pliers handle the conversation.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Example-3:-Face-detection,-frame-sampling,-and-chaining-Transformers">Example 3: Face detection, frame sampling, and chaining Transformers<a class="anchor-link" href="#Example-3:-Face-detection,-frame-sampling,-and-chaining-Transformers"> </a></h2>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>There are times when we need to specify the conversion parameters, and thus prefer not to rely on Pliers's automatic conversion. We can accomplish this by manually chaining a <code>Converter</code> to an <code>Extractor</code></p>
<p>In this example, we'll be using the <code>FaceRecognitionFaceLocationsExtractor</code> to detect faces in the <code>Sherlock</code> video stimulus. 
This extractor provides an interface to the <code>face_recognition</code> python package.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">pliers.extractors</span> <span class="k">import</span> <span class="n">FaceRecognitionFaceLocationsExtractor</span>

<span class="c1"># Using the more accurate &#39;cnn&#39; model, change this to &#39;hog&#39; for faster perfomance</span>
<span class="n">face_ext</span> <span class="o">=</span> <span class="n">FaceRecognitionFaceLocationsExtractor</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s1">&#39;cnn&#39;</span><span class="p">)</span>

<span class="c1"># This extractor expects ImageStim as input</span>
<span class="n">face_ext</span><span class="o">.</span><span class="n">_input_type</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>pliers.stimuli.image.ImageStim</pre>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>If we use this extractor to transform a <code>VideoStim</code>, <em>pliers</em> will implicitly use the <code>FrameSamplingConverter</code>.</p>
<p>However, it will do so with the default parameters, which in this case would extract every frame.
This is too fine grained, as we want to save ourselves the computation, and only sample infrequently for this example.</p>
<p>To convert the <code>Sherlock</code> stimuli from video, to <code>ImageStim</code> frames sampled at 0.1Hz (i.e., one frame every 10 seconds), we need to:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>First, load the <code>VideoStim</code>, and use the <code>FrameSamplingFilter</code> to subsample this set of <code>ImageStims</code> at 0.1hz.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">pliers.stimuli</span> <span class="k">import</span> <span class="n">VideoStim</span>
<span class="kn">from</span> <span class="nn">pliers.filters</span> <span class="k">import</span> <span class="n">FrameSamplingFilter</span>

<span class="n">video</span> <span class="o">=</span> <span class="n">VideoStim</span><span class="p">(</span><span class="n">sherlock_video</span><span class="p">)</span>

<span class="c1"># Sample at 0.1 Hz</span>
<span class="n">filt</span> <span class="o">=</span> <span class="n">FrameSamplingFilter</span><span class="p">(</span><span class="n">hertz</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">selected_frames</span> <span class="o">=</span> <span class="n">filt</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">video</span><span class="p">)</span>

<span class="c1"># Number of sampled frames</span>
<span class="n">selected_frames</span><span class="o">.</span><span class="n">n_frames</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>143</pre>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Note:
<code>FrameSamplingFilter</code> expects a <em>collection</em> of <code>ImageStims</code> as input, and returns a subsampled collection of <code>ImageStims</code>. However, here it can take <code>VideoStim</code> as input, as <em>pliers</em> will <em>impliclty</em> convert <code>VideoStim</code> -&gt; <code>ImageStim</code>. Since there are no important parameters to modify in this step, we can let <em>pliers</em> handle it for us, instead of doing it explicitly.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Next, we can use the <code>FaceRecognitionFaceLocationsExtractor</code> to detect and label face locations in the subset of frames</p>
<p>Note that since we transformed a collection of frames, the result of this operation is a <em>list</em> of <code>ExtractedResult</code> objects.</p>
<p>To merge these objects into a single pandas DataFrame, we can use the helper function <code>merge_results</code>.</p>
<p>Note that the following cell will take a few minutes to run (~4min, depending on your computer specs).</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">pliers.extractors</span> <span class="k">import</span> <span class="n">merge_results</span>
<span class="kn">from</span> <span class="nn">pliers</span> <span class="k">import</span> <span class="n">config</span>
<span class="c1"># Disable progress bar for Jupyter Book</span>
<span class="n">config</span><span class="o">.</span><span class="n">set_option</span><span class="p">(</span><span class="s1">&#39;progress_bar&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>

<span class="c1"># Detect faces in selected frames</span>
<span class="n">face_features</span> <span class="o">=</span> <span class="n">face_ext</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">selected_frames</span><span class="p">)</span>
<span class="n">merged_faces</span> <span class="o">=</span> <span class="n">merge_results</span><span class="p">(</span><span class="n">face_features</span><span class="p">,</span> <span class="n">metadata</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># Show only first few rows</span>
<span class="n">merged_faces</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">12</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>order</th>
      <th>object_id</th>
      <th>onset</th>
      <th>duration</th>
      <th>FaceRecognitionFaceLocationsExtractor#face_locations</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>NaN</td>
      <td>0</td>
      <td>60.0</td>
      <td>10.0</td>
      <td>(37, 415, 207, 245)</td>
    </tr>
    <tr>
      <th>1</th>
      <td>NaN</td>
      <td>0</td>
      <td>90.0</td>
      <td>10.0</td>
      <td>(57, 516, 97, 477)</td>
    </tr>
    <tr>
      <th>2</th>
      <td>NaN</td>
      <td>0</td>
      <td>100.0</td>
      <td>10.0</td>
      <td>(72, 570, 241, 400)</td>
    </tr>
    <tr>
      <th>3</th>
      <td>NaN</td>
      <td>0</td>
      <td>120.0</td>
      <td>10.0</td>
      <td>(49, 167, 117, 99)</td>
    </tr>
    <tr>
      <th>4</th>
      <td>NaN</td>
      <td>0</td>
      <td>130.0</td>
      <td>10.0</td>
      <td>(144, 466, 191, 419)</td>
    </tr>
    <tr>
      <th>5</th>
      <td>NaN</td>
      <td>0</td>
      <td>140.0</td>
      <td>10.0</td>
      <td>(102, 185, 159, 128)</td>
    </tr>
    <tr>
      <th>6</th>
      <td>NaN</td>
      <td>0</td>
      <td>150.0</td>
      <td>10.0</td>
      <td>(37, 260, 207, 90)</td>
    </tr>
    <tr>
      <th>7</th>
      <td>NaN</td>
      <td>0</td>
      <td>160.0</td>
      <td>10.0</td>
      <td>(8, 565, 252, 320)</td>
    </tr>
    <tr>
      <th>8</th>
      <td>NaN</td>
      <td>0</td>
      <td>210.0</td>
      <td>10.0</td>
      <td>(68, 433, 125, 376)</td>
    </tr>
    <tr>
      <th>9</th>
      <td>NaN</td>
      <td>0</td>
      <td>240.0</td>
      <td>10.0</td>
      <td>(125, 119, 193, 51)</td>
    </tr>
    <tr>
      <th>10</th>
      <td>NaN</td>
      <td>1</td>
      <td>240.0</td>
      <td>10.0</td>
      <td>(137, 341, 194, 284)</td>
    </tr>
    <tr>
      <th>11</th>
      <td>NaN</td>
      <td>2</td>
      <td>240.0</td>
      <td>10.0</td>
      <td>(125, 519, 182, 462)</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="n">merged_faces</span><span class="o">.</span><span class="n">onset</span><span class="o">.</span><span class="n">unique</span><span class="p">())</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>89</pre>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>There are 89 unique onsets, which indicates that faces were found in 89/143 frames.</p>
<p>The <code>FaceRecognitionFaceLocationsExtractor#face_locations</code> column also indicates the location of each face in CSS order (i.e., top, right, bottom, left).</p>
<p><strong>Tip:</strong> In some frames (e.g. 240s), multiple faces were found, and there are multiple rows for a given <code>onset</code>. To disambiguate these rows, <em>pliers</em> assigns each occurace a unique <code>object_id</code> column value. Read more](<a href="http://tyarkoni.github.io/pliers/results.html#understanding-object-ids">http://tyarkoni.github.io/pliers/results.html#understanding-object-ids</a>) about <em>object_id</em>.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now, Let's plot all of these frames where faces were found!</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Filter selected frames to only include those with faces</span>
<span class="c1"># and extract stimulus &quot;data&quot; (numpy representation of image frame) from the ExtractorResult object</span>
<span class="n">face_frames</span> <span class="o">=</span> <span class="p">[</span><span class="n">f</span><span class="o">.</span><span class="n">data</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">selected_frames</span> <span class="k">if</span> <span class="n">f</span><span class="o">.</span><span class="n">onset</span> <span class="ow">in</span> <span class="n">merged_faces</span><span class="o">.</span><span class="n">onset</span><span class="o">.</span><span class="n">tolist</span><span class="p">()]</span>
</pre></div>

</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">mpl_toolkits.axes_grid1</span> <span class="k">import</span> <span class="n">ImageGrid</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">plot_img_grid</span><span class="p">(</span><span class="n">img_list</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">30.</span><span class="p">,</span> <span class="mf">30.</span><span class="p">)):</span>
    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="n">figsize</span><span class="p">)</span>
    <span class="n">grid</span> <span class="o">=</span> <span class="n">ImageGrid</span><span class="p">(</span><span class="n">fig</span><span class="p">,</span> <span class="mi">111</span><span class="p">,</span> <span class="n">nrows_ncols</span><span class="o">=</span><span class="n">shape</span><span class="p">,</span> <span class="n">axes_pad</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="n">im</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">img_list</span><span class="p">):</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">im</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">plot_img_grid</span><span class="p">(</span><span class="n">face_frames</span><span class="p">,</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">9</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="../../images/features/notebooks/Pliers_Tutorial_45_0.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now, let's take a look at the frames <em>without</em> a detected face:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">nonface_frames</span> <span class="o">=</span> <span class="p">[</span><span class="n">f</span><span class="o">.</span><span class="n">data</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">selected_frames</span> <span class="k">if</span> <span class="n">f</span><span class="o">.</span><span class="n">onset</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">merged_faces</span><span class="o">.</span><span class="n">onset</span><span class="o">.</span><span class="n">tolist</span><span class="p">()]</span>
</pre></div>

</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">plot_img_grid</span><span class="p">(</span><span class="n">nonface_frames</span><span class="p">,</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">9</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="../../images/features/notebooks/Pliers_Tutorial_48_0.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Great, for the most part it looks like the algorithm corrently idenitfied frames with visble faces, with occasional misses when faces were small.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="When-to-use-implicit-conversion?">When to use implicit conversion?<a class="anchor-link" href="#When-to-use-implicit-conversion?"> </a></h2><p>Now that you know how to manually convert stimuli, you may wonder, when should I use manual conversion, and when should I rely on <em>implicit conversion</em>?</p>
<p>Pliers will always attempt implicit conversion when there is a mismatch between the input, and the expected input to a <code>Transformer</code>. If in doubt, inspect the <code>_input_type</code> attribute of an <code>Extractor</code>.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">face_ext</span><span class="o">.</span><span class="n">_input_type</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>pliers.stimuli.image.ImageStim</pre>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><code>ExtractedResult</code> objects also keep a history of what was done prior to Extraction, which you can always inspect</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">str</span><span class="p">(</span><span class="n">face_features</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">history</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>&#39;VideoStim-&gt;FrameSamplingFilter/VideoFrameCollectionStim-&gt;VideoFrameCollectionIterator/VideoFrameStim-&gt;FaceRecognitionFaceLocationsExtractor/ExtractorResult&#39;</pre>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Note:</strong>
You can learn more about Transformation history in the <a href="http://tyarkoni.github.io/pliers/stimuli.html#transformation-history">pliers docs</a>.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>If in doubt, simply chain <code>Converters</code> to <code>Extractors</code> manually.</p>
<p>If you want to learn more implict conversion, this topic is discussed in the <a href="http://tyarkoni.github.io/pliers/transformers.html#implicit-stim-conversion">Pliers documentation</a>.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Speech-to-text-Converters">Speech-to-text Converters<a class="anchor-link" href="#Speech-to-text-Converters"> </a></h1>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>So far, we have experimented with extracting features from <code>AudioStim</code> and <code>VideoStim</code>s. However, many naturalistic datasets feature extensive human speech, which likely drives a substantial amount of brain activity.</p>
<p>However, to analyze the speech language, we need a precisely aligned transcript of speech.</p>
<p>Recent advancements in deep convolutional networks have spawned dozens of <code>speech-to-text</code> language models, that promise accurate, time-locked transcripts of speech. The best available models are currently paid services that can be accessed remotely using APIs. <em>Pliers</em> has implemented support for several <code>speech-to-text</code> services (as well as image analysis APIs). For this example, we'll be using a service called <code>Rev.ai</code> to transcript run 1 of Paranoia Story to text.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Example-4:-Speech-to-Text-Conversion-with-Rev.ai">Example 4: Speech-to-Text Conversion with Rev.ai<a class="anchor-link" href="#Example-4:-Speech-to-Text-Conversion-with-Rev.ai"> </a></h2>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Remote-service-API-Credentials">Remote service API Credentials<a class="anchor-link" href="#Remote-service-API-Credentials"> </a></h3><p>To use remote APIs, we need to give <em>pliers</em> our private credentials for these paid services, by setting the appropriate environment variable in our environment to the access key. For more details, <a href="http://tyarkoni.github.io/pliers/installation.html#api-keys">see here</a></p>
<p>For the sake of this tutorial, you can sign up for a free trial of <a href="https://rev.ai">rev.ai</a> (5 hours free, no CC required). We have also provided a pre-extracted transcript that you can load instead.</p>
<p>Once signed up visit <a href="https://www.rev.ai/access_token">https://www.rev.ai/access_token</a>_ to receive an access token, and set your token to the environment using the following command:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;REVAI_ACCESS_TOKEN&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;PRIVATE_KEY&#39;</span> <span class="c1"># replace PRIVATE_KEY with your own key!</span>
</pre></div>

</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Warning!</strong>
 Never share this token or push to GitHub!
```</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Although we're using a machine-learning model to transcribe our audio file, <em>pliers</em> considers this a <code>Converter</code>, as we transform an <code>AudioStim</code> to a <code>ComplexTextStim</code>.</p>
<p>Let's try it out:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">pliers.converters</span> <span class="k">import</span> <span class="n">RevAISpeechAPIConverter</span>
<span class="n">ext</span> <span class="o">=</span> <span class="n">RevAISpeechAPIConverter</span><span class="p">()</span>

<span class="c1"># Uncomment the next two lines if you have set up the Rev.AI keys</span>
<span class="c1"># transcript = ext.transform(paranoia_audio)</span>
<span class="c1"># transcript.save(data_dir_paranoia + &#39;stimuli/paranoia_story1_transcript.txt&#39;)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>WARNING:root:Beginning audio transcription with a timeout of 1000.000000s. Even for small audios, full transcription may take awhile.
</pre>
</div>
</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># For the sake of the demonstration, let&#39;s load an pre-existing transcript</span>
<span class="kn">from</span> <span class="nn">pliers.stimuli</span> <span class="k">import</span> <span class="n">ComplexTextStim</span>
<span class="n">transcript_file</span> <span class="o">=</span> <span class="n">data_dir_paranoia</span> <span class="o">+</span> <span class="s1">&#39;stimuli/paranoia_story1_transcript.txt&#39;</span>
<span class="n">transcript</span> <span class="o">=</span> <span class="n">ComplexTextStim</span><span class="p">(</span><span class="n">transcript_file</span><span class="p">)</span>
<span class="n">transcript</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>&lt;pliers.stimuli.text.ComplexTextStim at 0x1478a1cc0&gt;</pre>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>As you can see, the <code>type</code> of <code>transcript</code> is <code>ComplexTextStim</code>. A <code>ComplexTextStim</code> is simply a container object that is composed of several <code>TextStim</code>s which represent individual words. Each individual <code>TextStim</code> element can then be associated with individual onsets and/or durations</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># First first elements of the transcript</span>
<span class="n">transcript</span><span class="o">.</span><span class="n">elements</span><span class="p">[:</span><span class="mi">5</span><span class="p">]</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>[&lt;pliers.stimuli.text.TextStim at 0x1478a1710&gt;,
 &lt;pliers.stimuli.text.TextStim at 0x1478a1ac8&gt;,
 &lt;pliers.stimuli.text.TextStim at 0x1478a1588&gt;,
 &lt;pliers.stimuli.text.TextStim at 0x1478a1c18&gt;,
 &lt;pliers.stimuli.text.TextStim at 0x1478a1ba8&gt;]</pre>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># First word of the transcript</span>
<span class="n">transcript</span><span class="o">.</span><span class="n">elements</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">text</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>&#39;The&#39;</pre>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Specific onset for this word</span>
<span class="n">transcript</span><span class="o">.</span><span class="n">elements</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">onset</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>2.09</pre>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">

<pre><code>{tip}
*pliers* by default caches the result of `Extractors` in the background. If we were to re-run this `RevAISpeechAPIConverter` a second time on the same stimulus, it will simply used the cached results rather than perform a potentially expensive repeated API call. This caching behavior applies to all `Transformers` and can save needly re-extraction.</code></pre>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's take a closer look at the transcript by loading it as pandas DataFrame.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="n">pd</span><span class="o">.</span><span class="n">read_table</span><span class="p">(</span><span class="n">transcript_file</span><span class="p">)</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">15</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>onset</th>
      <th>text</th>
      <th>duration</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2.09</td>
      <td>The</td>
      <td>0.21</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2.30</td>
      <td>email</td>
      <td>0.36</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2.66</td>
      <td>came</td>
      <td>0.42</td>
    </tr>
    <tr>
      <th>3</th>
      <td>3.11</td>
      <td>late</td>
      <td>0.27</td>
    </tr>
    <tr>
      <th>4</th>
      <td>3.38</td>
      <td>one</td>
      <td>0.24</td>
    </tr>
    <tr>
      <th>5</th>
      <td>3.62</td>
      <td>afternoon</td>
      <td>0.69</td>
    </tr>
    <tr>
      <th>6</th>
      <td>4.55</td>
      <td>as</td>
      <td>0.18</td>
    </tr>
    <tr>
      <th>7</th>
      <td>4.73</td>
      <td>dr</td>
      <td>0.39</td>
    </tr>
    <tr>
      <th>8</th>
      <td>5.12</td>
      <td>Carmen</td>
      <td>0.39</td>
    </tr>
    <tr>
      <th>9</th>
      <td>5.51</td>
      <td>Reed</td>
      <td>0.33</td>
    </tr>
    <tr>
      <th>10</th>
      <td>5.84</td>
      <td>was</td>
      <td>0.18</td>
    </tr>
    <tr>
      <th>11</th>
      <td>6.02</td>
      <td>sitting</td>
      <td>0.36</td>
    </tr>
    <tr>
      <th>12</th>
      <td>6.38</td>
      <td>in</td>
      <td>0.15</td>
    </tr>
    <tr>
      <th>13</th>
      <td>6.53</td>
      <td>her</td>
      <td>0.15</td>
    </tr>
    <tr>
      <th>14</th>
      <td>6.68</td>
      <td>office</td>
      <td>0.51</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In just a few minutes, and a few lines of code, we have a fairly accurate, precisely aligned transcript for <code>ParanoiaStory</code>.</p>
<p>Now, what can we do with it?</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Example-5:-Text-based-Extractors">Example 5: Text-based Extractors<a class="anchor-link" href="#Example-5:-Text-based-Extractors"> </a></h2>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><em>Pliers</em> offers support for dozens of <code>Extractors</code> that operate on <code>TextStims</code>.</p>
<p>A light-weight and surprsingly useful approach is simply to annotate words using Dictionaries of established word norms. <em>Pliers</em> makes this is easy using the <code>DictionaryExtractor</code>.</p>
<p>A <code>DictionaryExtractor</code> simply maps words to arbitrary features encoded in a user-provided look up table. <em>Pliers</em> comes with built-in support for several word-norm mappings using the <a href="http://tyarkoni.github.io/pliers/generated/pliers.extractors.PredefinedDictionaryExtractor.html#pliers.extractors.PredefinedDictionaryExtractor">PredefinedDictionaryExtractor</a>.</p>
<p>The predefined dictionaries built-in to <em>pliers</em> can be seen in the <a href="https://github.com/tyarkoni/pliers/blob/master/pliers/datasets/dictionaries.json">repository</a>.</p>
<p>In this example, we're going to extract values for age-of-acquisition <a href="https://link.springer.com/article/10.3758/s13428-012-0210-4">(Kuperman et al., 2012)</a> and affective valence <a href="https://link.springer.com/article/10.3758/s13428-012-0314-x">(Warriner et al., 2013)</a>.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">pliers.extractors</span> <span class="k">import</span> <span class="n">PredefinedDictionaryExtractor</span>

<span class="n">dictext</span> <span class="o">=</span> <span class="n">PredefinedDictionaryExtractor</span><span class="p">(</span>
    <span class="n">variables</span><span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;affect&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;V.Mean.Sum&#39;</span><span class="p">],</span>
        <span class="s2">&quot;aoa&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;AoA_Kup&quot;</span><span class="p">]</span>
    <span class="p">})</span>

<span class="n">norms_results</span> <span class="o">=</span> <span class="n">dictext</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">transcript</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">pliers.extractors</span> <span class="k">import</span> <span class="n">merge_results</span>
</pre></div>

</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">norms_df</span> <span class="o">=</span> <span class="n">merge_results</span><span class="p">(</span><span class="n">norms_results</span><span class="p">)</span>
<span class="n">norms_df</span><span class="p">[[</span><span class="s1">&#39;stim_name&#39;</span><span class="p">,</span> <span class="s1">&#39;onset&#39;</span><span class="p">,</span> <span class="s1">&#39;PredefinedDictionaryExtractor#affect_V.Mean.Sum&#39;</span><span class="p">,</span> 
          <span class="s1">&#39;PredefinedDictionaryExtractor#aoa_AoA_Kup&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">15</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>stim_name</th>
      <th>onset</th>
      <th>PredefinedDictionaryExtractor#affect_V.Mean.Sum</th>
      <th>PredefinedDictionaryExtractor#aoa_AoA_Kup</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>text[email]</td>
      <td>2.30</td>
      <td>6.00</td>
      <td>11.520000</td>
    </tr>
    <tr>
      <th>1</th>
      <td>text[late]</td>
      <td>3.11</td>
      <td>3.32</td>
      <td>5.280000</td>
    </tr>
    <tr>
      <th>2</th>
      <td>text[one]</td>
      <td>3.38</td>
      <td>6.09</td>
      <td>3.227100</td>
    </tr>
    <tr>
      <th>3</th>
      <td>text[afternoon]</td>
      <td>3.62</td>
      <td>6.68</td>
      <td>4.650000</td>
    </tr>
    <tr>
      <th>4</th>
      <td>text[as]</td>
      <td>4.55</td>
      <td>NaN</td>
      <td>6.104490</td>
    </tr>
    <tr>
      <th>5</th>
      <td>text[in]</td>
      <td>6.38</td>
      <td>NaN</td>
      <td>3.685351</td>
    </tr>
    <tr>
      <th>6</th>
      <td>text[her]</td>
      <td>6.53</td>
      <td>NaN</td>
      <td>5.092075</td>
    </tr>
    <tr>
      <th>7</th>
      <td>text[office]</td>
      <td>6.68</td>
      <td>4.54</td>
      <td>6.680000</td>
    </tr>
    <tr>
      <th>8</th>
      <td>text[filling]</td>
      <td>7.43</td>
      <td>NaN</td>
      <td>6.818509</td>
    </tr>
    <tr>
      <th>9</th>
      <td>text[out]</td>
      <td>7.76</td>
      <td>NaN</td>
      <td>3.280385</td>
    </tr>
    <tr>
      <th>10</th>
      <td>text[medical]</td>
      <td>7.97</td>
      <td>5.22</td>
      <td>8.680000</td>
    </tr>
    <tr>
      <th>11</th>
      <td>text[for]</td>
      <td>8.81</td>
      <td>NaN</td>
      <td>4.388713</td>
    </tr>
    <tr>
      <th>12</th>
      <td>text[the]</td>
      <td>8.99</td>
      <td>NaN</td>
      <td>3.983747</td>
    </tr>
    <tr>
      <th>13</th>
      <td>text[she]</td>
      <td>9.65</td>
      <td>NaN</td>
      <td>3.568124</td>
    </tr>
    <tr>
      <th>14</th>
      <td>text[that]</td>
      <td>10.34</td>
      <td>NaN</td>
      <td>5.529012</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In a single dataframe, we can see the input words, and the values for each of the two norms we requested.</p>

<pre><code>{note}
If a Dictionary does not include the requested word, it will be encoded as `NaN` value. If both dictionaries are missing the word, it will be dropped when merging.</code></pre>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Graph-API:-Managing-complex-workflows">Graph API: Managing complex workflows<a class="anchor-link" href="#Graph-API:-Managing-complex-workflows"> </a></h1>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Congratulations</strong> At this point, you should now enough to take advantage of the majority <em>pliers</em> has to offer.</p>
<p>But, with great power comes great responsibilty... or rather, complex workflows.</p>
<p>Say you want to extract various features on the <code>Sherlock</code> dataset, which require distinct conversion steps. You could do as we have so far, and string together various <code>Transformers</code> and manually connect them, but this can become cumbersome, and verbose.</p>
<p>For example, suppose we want to extract both visual and speech transcript-based features from the <code>Sherlock</code> dataset. Specifically, let’s say we want to encode the visual brightness of each frame and detect faces. We also run a sentiment analysis model on a speech transcription extracted from the audio track of the videos. We also want to extract frequency norms on each transcribed word. This requires us to do all of the following:</p>
<ul>
<li>Convert the video to a series of video frames, using FrameSamplingFilter;</li>
<li>Run a brightness extractor on each image frame;</li>
<li>Detect faces for each frame;</li>
<li>Extract the audio track from the video;</li>
<li>Transcribe the audio track to text;</li>
<li>Run a sentiment analysis model (in this case, using the Vander sentiment extractor) on the transcribed text.</li>
<li>Run a predefined dictionary extractor on the transcribed text</li>
</ul>
<p>Using the normal <em>pliers</em> API, the code to run these steps would look like this:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">

<pre><code>from pliers.stimuli import VideoStim
from pliers.filters import FrameSamplingFilter
from pliers.converters import RevAISpeechAPIConverter
from pliers.extractors import (VADERSentimentExtractor, FaceRecognitionFaceLandmarksExtractor)

stimulus = VideoStim(sherlock_video)

# Sample 2 video frames / second
frame_filter = FrameSamplingFilter(hertz=0.1)
frames = frame_filter.transform(stimulus)

# Face extraction
face_ext = FaceRecognitionFaceLandmarksExtractor()
face_results = face_ext.transform(frames)

# STFT Extraction
stft_ext = STFTAudioExtractor(freq_bins=[(100, 300)])
stft_res = stft_ext.transform(stimulus)

# Run the audio through RevAI's text detection API
transcriber = RevAISpeechAPIConverter()
transcripts = transcriber.transform(stimulus)

# Apply Vader sentiment analysis extractor
sentiment = VADERSentimentExtractor()
sentiment_results = sentiment.transform(transcripts)

# Apply PredefinedDictionaryExtractor
dict_ext = PredefinedDictionaryExtractor(['affect/V.Mean.Sum',
                                         'subtlexusfrequency/Lg10WF'])
dict_results = dict_ext.transform(transcripts)

# Combine visual and text-based feature data
results = face_results + [stft_res] + sentiment_results + dict_results

# # Merge into a single pandas DF
df = merge_results(results)</code></pre>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The above code really isn’t that bad -- it already features a high level of abstraction (each <code>Transformer</code> is initialized and applied in just two lines of code!), and has the advantage of being explicit about every step. Nonetheless, if we want to save ourselves a few dozen keystrokes, we can use <em>pliers’</em> <code>Graph</code> API to abbreviate the listing down to just this:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">

<pre><code>from pliers.graph import Graph

# Define nodes
nodes = [
    (FrameSamplingFilter(hertz=0.1),
         ['FaceRecognitionFaceLandmarksExtractor', 'BrightnessExtractor']),
    (STFTAudioExtractor(freq_bins=[(100, 300)])),
    ('RevAISpeechAPIConverter',
         [PredefinedDictionaryExtractor(['affect/V.Mean.Sum','subtlexusfrequency/Lg10WF']),
          'VADERSentimentExtractor'])
]

# Initialize and execute Graph
g = Graph(nodes)

# Arguments to merge_results can be passed in here
df = g.transform(sherlock_video)</code></pre>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This short example demostrates a powerful way to express complex extraction workflows in <em>pliers</em>.</p>
<p>If you'd like to learn more about the <code>Graph</code> API, you can head over to the <a href="http://tyarkoni.github.io/pliers/graphs.html">documentation</a>.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Where-to-go-from-here?">Where to go from here?<a class="anchor-link" href="#Where-to-go-from-here?"> </a></h1>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><em>Pliers</em> is a powerful, yet easy to use Python library for multi-modal feature extraction. In this short tutorial, you've seen how to use <em>pliers</em> to modify, convert and extract features from stimuli.</p>
<p>To see the full range of <code>Transformers</code> implemented in <em>pliers</em>, <a href="http://tyarkoni.github.io/pliers/transformers.html">refer to this listing</a></p>
<p>What's most exciting about <em>pliers</em>, and the quickly evolving landscape of feature algorithms is that they are constantly evolving. New developments promise to allows to extract ever more psychologically relevant features from naturalistic stimuli, automatically.</p>
<p>To that end, <em>pliers</em> is designed to be easily expanded by open-source contributions. If you have any ideas for future extractors, or simply find a bug to report, head over to our GitHub repository and <a href="https://github.com/tyarkoni/pliers/issues">file and issue</a>.</p>
<p>If you use pliers in your work, please cite both the <a href="http://github.com/tyarkoni/pliers">pliers GitHub repository</a> and the following paper:</p>
<ul>
<li>McNamara, Q., De La Vega, A., &amp; Yarkoni, T. (2017, August). Developing a comprehensive framework for multimodal feature extraction. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (pp. 1567-1574). ACM. <a href="https://dl.acm.org/doi/abs/10.1145/3097983.3098075?casa_token=iXf9sdY9XdQAAAAA%3Abu4w3Um9wJr3_c4eEtJ8nCdLzWYSGQ7Fmg4KM6N0uCQ3u-Jryvk3lK0JLvQJFcReIEqiUksZWBAl">link</a></li>
</ul>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="References">References<a class="anchor-link" href="#References"> </a></h1>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li><p>Kuperman, V., Stadthagen-Gonzalez, H. &amp; Brysbaert, M. Age-of-acquisition ratings for 30,000 English words. Behav Res 44, 978–990 (2012). <a href="https://doi.org/10.3758/s13428-012-0210-4">https://doi.org/10.3758/s13428-012-0210-4</a></p>
</li>
<li><p>Warriner, A.B., Kuperman, V. &amp; Brysbaert, M. Norms of valence, arousal, and dominance for 13,915 English lemmas. Behav Res 45, 1191–1207 (2013). <a href="https://doi.org/10.3758/s13428-012-0314-x">https://doi.org/10.3758/s13428-012-0314-x</a></p>
</li>
</ul>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Contributions">Contributions<a class="anchor-link" href="#Contributions"> </a></h1>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Alejandro de la Vega wrote the tutorial and code in this notebook. Emily Finn tested and lightly edited the notebook.</p>

</div>
</div>
</div>
</div>

 


    </main>
    